{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4462a7-6645-42f8-8f38-2740c28f318e",
   "metadata": {},
   "source": [
    "# Slice and Dice ICP data\n",
    "\n",
    "Using Pandas to import these csv files and then organize by element name and wavelength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476964f8-bdbf-4f5d-8af5-45d40491c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a strategy for importing data into lists so that we can use it!\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import math\n",
    "# read your data file in here (update the file name to match yours!):\n",
    "data = pd.read_csv(\"FA22_exp4.csv\", header = 6)\n",
    "\n",
    "\n",
    "#slice by element\n",
    "Cu = data[data.values == 'Cu']\n",
    "Pb = data[data.values == 'Pb']\n",
    "Fe = data[data.values == 'Fe']\n",
    "\n",
    "#slice out standards for each element\n",
    "Cu_standards = Cu[Cu.values == 'STD'] #all standard data for Cu\n",
    "Pb_standards = Pb[Pb.values == 'STD'] #all standard data for Pb\n",
    "Fe_standards = Fe[Fe.values == 'STD'] #all standard data for Fe\n",
    "\n",
    "standards_conc = [0.1,0.5,1.0,2.0,4.0,6.0,8.0,10.0,25.0,50.0] #values entered in ug/L, make sure this is the correct order for our data!\n",
    "\n",
    "\n",
    "\n",
    "#pull out all average intensities\n",
    "Cu_standard_x = Cu_standards['Intensity'].tolist()\n",
    "Pb_standard_x = Pb_standards['Intensity'].tolist()\n",
    "Fe_standard_x = Fe_standards['Intensity'].tolist()\n",
    "\n",
    "#grab standard deviations from the data\n",
    "Cu_standards_s = Cu_standards['Intensity SD'].tolist()\n",
    "Pb_standard_x = Pb_standards['Intensity SD'].tolist()\n",
    "Fe_standard_x = Fe_standards['Intensity SD'].tolist()\n",
    "\n",
    "# grab all unknown replicates\n",
    "\n",
    "Cu_unknowns = Cu[Cu.values == 'Sample'] # all the data\n",
    "Cu_unknown_x = Cu_unknowns['Intensity'].tolist() # intensity data for each unknown\n",
    "Cu_unknown_s = Cu_unknowns['Intensity SD'].tolist() # standard deviations for each unknown\n",
    "\n",
    "Pb_unknowns = Pb[Pb.values == 'Sample'] # all the data\n",
    "Pb_unknown_x = Pb_unknowns['Intensity'].tolist() # intensity data for each unknown\n",
    "Pb_unknown_s = Pb_unknowns['Intensity SD'].tolist() # standard deviations for each unknown\n",
    "\n",
    "Fe_unknowns = Fe[Fe.values == 'Sample'] # all the data\n",
    "Fe_unknown_x = Fe_unknowns['Intensity'].tolist() # intensity data for each unknown\n",
    "Fe_unknown_s = Fe_unknowns['Intensity SD'].tolist() # standard deviations for each unknown\n",
    "\n",
    "#print(Cu)\n",
    "#print(Pb)\n",
    "#print(Fe)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc095f3-33ef-410b-94d2-8835d42aa818",
   "metadata": {},
   "source": [
    "## Part 1 - Graph the data and add error bars\n",
    "\n",
    "For now, don't change anything about how this data is reading in, just make sure the code above has been run, and then let's plot the graph.\n",
    "You've seen this code before, but take a closer look at what it's doing right now:\n",
    "plt.plot is the command to plot an x-y scatter plot. plt.plot(x-value list, y-value list) is the simplest format we can use. The next information added here is 'bx' which gives us blue x marks for our data points. We can change that to change the look of our plot. Options can be found in the matplotlib documentation. \n",
    "All of the possible markers are listed here. For ease of use, stick to markers that are given inside apostrophes, which will work most easily in our simplified format https://matplotlib.org/stable/gallery/lines_bars_and_markers/marker_reference.html\n",
    "All of the named colors are listed here, again, prioritize the use of those listed in apostrophes: https://matplotlib.org/stable/gallery/color/named_colors.html#base-colors\n",
    "\n",
    "Change the color and marker shape to try this out, and be sure to fix the axis labels!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0104aa00-7d83-4bfb-99ac-72ed78613b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(standards_conc, Cu_standard_x, 'bx')\n",
    "\n",
    "# Add reasonable labels on the x and y axis, always including units.\n",
    "plt.xlabel(\"tacos (units)\")\n",
    "plt.ylabel(\"happiness (are there units here too?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c83fda-baa9-418f-ac61-03972d2e2356",
   "metadata": {},
   "source": [
    "## Adding error bars to our plot\n",
    "\n",
    "We might consider using the standard deviation as a way to communicate some uncertainty in each of these values. We can do that by adding error bars to the plot!\n",
    "\n",
    "Standard deviation is a very blunt instrument for expressing error in this case, but it is better than nothing. Let's try that first.\n",
    "Notice we've switched to a different type of plt plot, and so we've needed to add some additional descriptors to keep track of what each of our inputs are doing.\n",
    "The format is still plot.errorbar(x,y) but then we need to explicity give values for yerror bars (yerr), xerror bars (xerr, here none, since we don't have values there) and then we use fmt as format, to inidcate the color and shape of the markers). We could omit these descriptors and the code would still work, but leaving them in helps us keep track of what is happening as the functions get more complex!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c56b41-c5db-4370-8fa6-5af1cbbbfd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(standards_conc, Cu_standard_x, yerr = Cu_standards_s, xerr = None, fmt = 'r.')\n",
    "plt.xlabel(\"concentration (ug/L)\")\n",
    "plt.ylabel(\"average intensity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bf2742-f4ad-4901-9e9f-bac6e42115dc",
   "metadata": {},
   "source": [
    "We can't actually see much here; the standard deviations are very small. It is best to use the confidence interval for error bars instead! Remember, we just need to multiple our standard deviation by a Student's t constant, and divide by the square root of n. Here, the code is written to show error bars at the 95 % confidence interval. Change the confidence level until you can actually see the error bars!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad712bc-5616-4720-b67a-27982e3fc253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use c to set the confidence you want (90 % is 0.90, for example)\n",
    "c = 0.90\n",
    "alpha = 1-c\n",
    "\n",
    "#how many replicates did we run?\n",
    "n = 3\n",
    "\n",
    "t = stats.t.ppf(1-alpha/2, n-1)\n",
    "\n",
    "CI = np.array(Cu_standards_s)*(t/math.sqrt(n))\n",
    "\n",
    "plt.errorbar(standards_conc, Cu_standard_x, yerr = CI, xerr = None, fmt = 'r.')\n",
    "plt.xlabel(\"concentration (ug/L)\")\n",
    "plt.ylabel(\"average intensity\")\n",
    "\n",
    "#change the confidence level until you can actual see error bars!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244340aa-88be-4528-9f5a-aa75e75fc998",
   "metadata": {},
   "source": [
    "## External Calibration Curve\n",
    "### Linear Regression Model and Predicting the Equation of the Line\n",
    "\n",
    "This type of plot is called an <b> external calibration curve </b> in analytical chemistry, because we can take some other (external) data point, for which we have, say, a density measurement, and use this plot to determine that unknown sample's concentration. You have seen this before in general chemistry (for example, you may have plotted absorbance vs. concentration Ni in 152L)\n",
    "\n",
    "\n",
    "To get the most out of an external calibration curve, it's helpful to have the equation of the line, in <i> y = mx + b </i> format. That allows you to quickly and easily do the math to determine any <i> x </i> (here, concentration in mole/liter) for any measured <i> y </i> (here, density). You may know how to do this in Excel, or on your graphing calculator. Let's take a look at how it works in Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a19b0-4805-4b58-9a2a-d96ed0c82a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the linear regression function in the scipy stats module returns 5 values: slope, intercept, R-squared and then two uncertainty values p and s_m\n",
    "# we'll ignore the last two for the moment, since all we really need right now is the equation of the line\n",
    "# you may want to update this to keep track of which element we're talking about here!\n",
    "m, b, R2, p, s_m = stats.linregress(standards_conc, Cu_standard_x)\n",
    "\n",
    "print (F'the equation of the line is y = {m}x + {b} and its R-squared value is {R2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba9fbd9-d3c5-4d51-b4b9-f9a358b3ad9a",
   "metadata": {},
   "source": [
    "### Adding the 'best fit line' to your graph\n",
    "\n",
    "It's always a nice reality check to see the line plotted along with your data. In this case, we can think about this line as a model to help us make a prediction about our data. So we can plug in a range of concentration values, and we so we can plot the y values that our y = mx + b model predicts, and see how well it matches the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15eb52-c4a3-457f-a7a4-bc3f97ce2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we do need to convert standards to a numpy array in order to make sure we can multiple through for each value\n",
    "x = np.array(standards_conc)\n",
    "\n",
    "# Use our predicted model\n",
    "predict_y = (x*m)+b\n",
    "# Plot the predicted data (note if we don't ask for a specific marker style, we'll just get a line)\n",
    "plt.plot(x,predict_y)\n",
    "\n",
    "#add the real data to the plot, just like we did before:\n",
    "plt.errorbar(standards_conc, Cu_standard_x, yerr = CI, xerr = None, fmt = 'r.')\n",
    "plt.xlabel(\"concentration ug/mL\")\n",
    "plt.ylabel(\"intensity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5401f9-d38f-4f53-93f0-7fcab6209da7",
   "metadata": {},
   "source": [
    "## Solve for your unknown\n",
    "\n",
    "Using the equation above, calculate the first unknown concentration by hand . Then complete the code below to calculate the unknown concentration. Make sure your hand-written calculation and your python code are producing the same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157904e4-57c3-4c19-900f-271b4e2f99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists to numpy arrays so we can do math on each element individually\n",
    "unknown_av = np.array(Cu_unknown_x)\n",
    "\n",
    "unknown_std = np.array(Cu_unknown_s)\n",
    "\n",
    "# now using m, b and unknown_density, solve for unknown_concentration\n",
    "unknown_concentration = (unknown_av-b)/m\n",
    "\n",
    "print (F'the concentration of the unknowns are {unknown_concentration} ug/mL')\n",
    "\n",
    "#do your values match? If not, check your math!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a489446-b172-4c9a-a6be-80c5f5bd7a9e",
   "metadata": {},
   "source": [
    "## Uncertainty in a linear regression\n",
    "\n",
    "We have an $R^{2}$ value which gives us an idea how well our predicted line is able to fit our real data, but it's hard to turn that value into a real uncertainty value on our unknown calcultion. Ideally, we'd like an uncertainty in the same units are our final answer here (so in this case, in ug/L). In order to do that, we need to think about what error actually means in a linear regression.\n",
    "\n",
    "You read a lot of detail about how the matrix math works to produce that linear regression, and along with it, error values in the slope and intercept values produced. For whatever reason, most of the python linear regression packages just return the error in the slope. This is useful if you're main goal is to use a linear regression to determine a relationship between your variables, but in our case, we really need the error in the y values being predicted. That is the value we can transform into an x value error!\n",
    "\n",
    "Think about what error on the y-value means. You're making a prediction, so our questions is how close is that prediction to the real value?\n",
    "\n",
    "First, we'll think about the deviation of each measured y value ($y_{i}$) from the predicted y value (where $ y= mx+b $):\n",
    "\n",
    "$$ d_{i} = y_{i} - (m x_{i} + b) $$\n",
    "\n",
    "Then we want to compile those deviations for every point we have available, to turn them into an overall assessment of the standard deviation of the y values:\n",
    "\n",
    "$$ s_{y}={\\sqrt {\\frac {\\sum (d_{i})^{2}}{n-2}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a35eb97-6aab-4a8b-91ff-e3478e123d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculate error in y \n",
    "# standard error in most python packages is the error in the slope, rather than error in the y-predicitions of the model\n",
    "#organize our x and y data here, starting with Cu\n",
    "x = np.array(standards_conc)\n",
    "y = np.array(Cu_standard_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21261755-ab41-4aaa-b17f-a96a490ed0ca",
   "metadata": {},
   "source": [
    "### Standard error in the intercept\n",
    "\n",
    "Since the linear regression command in the stats module gives us the uncertainty in the slope, and we just calculated the uncertainty in the y values, we can take a shortcut to the uncertainty in the intercept, which avoids having to actually do any matrix math here. Note that\n",
    "\n",
    "$$ u_{m}^{2} = \\frac{s_{y}^{2}n}{D} $$\n",
    "\n",
    "Can be rewritten as $$ D = \\frac{s_{y}^{2}n}{u_{m}^{2}} $$\n",
    "\n",
    "So that we can write $ u_{b}^{2} $ in terms of values we have already calculated!\n",
    "$$ u_{b}^{2} = \\frac{s_{y}^{2}\\sum{x_{i}^{2}}}{D} = \\frac{\\sum{x_{i}^{2}}u_{m}^{2}}{n} $$\n",
    "\n",
    "Once you have all of these uncertainty values, be sure to print out equation of the line, with full uncertainty and correct sig figs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525083e-f881-49f6-8bee-b850f8f92478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d86c841b-f4be-4f2d-b948-c2ebe96cadae",
   "metadata": {},
   "source": [
    "### Convert a y value into an uncertainty on our calculated x value\n",
    "\n",
    "Now we have all of the information we need to propogate error through the calibration curve. The equation needed is shown below:\n",
    "\n",
    "$$ s_{x}= \\frac{s_{y}}{\\mid{m}\\mid}{\\sqrt {\\frac {1}{k} + \\frac {1}{n} + \\frac {(y-\\bar{y})^{2}}{m^{2}\\sum (x-\\bar{x})^{2}}}} $$\n",
    "\n",
    "You now have all of these variables:  m is the slope, k is the number of replicate measurements in your unknown, n is the number of points in your calibration curve, and x is all of your calibration curve x values, either individually ($ x_{i} $) or the average of those values $ (\\bar{x} ) $ \n",
    "\n",
    "$\\bar{y}$ is the average of all of the y values in your calibration curve, and y is the unknown you measured. Note that this means that the closer your measured unknown is to the center of your calibration curve, the smaller that value will be!\n",
    "\n",
    "Now we have to set up some math to make this happen. I'd strongly suggest breaking it down into components!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489555af-e7a8-45c7-9920-3ccb0317d285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f0e339f-461c-4312-b896-bedef114685b",
   "metadata": {},
   "source": [
    "## Limit of Detection and Limit of Quantitation\n",
    "\n",
    "We can use data from any external calibration curve to estimate a LOD and LOQ for our technique. In this case, the signal LOD is the intensity given by $$ b+3 * y_{error} $$\n",
    "Plugging into y = mx+ b to get an LOD in concentration units gives: $$ x_{LOD} = \\frac{3*y_{error}}{m} $$\n",
    "\n",
    "LOQ is the same idea, but with a higher threshold of error (so that quantitative data is reported with error less than 10%!)\n",
    "$$ x_{LOQ} = \\frac{10*y_{error}}{m} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de561d6-32b1-489f-a49a-6e9acc7d7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of the variables needed here are already defined!\n",
    "# Calculate LOD for each element:\n",
    "\n",
    "\n",
    "# Calculate LOQ for each element:\n",
    "\n",
    "\n",
    "\n",
    "# Print the values out nicely with descriptions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e77d1f8-488f-4853-b4c5-eb8f7efda7f2",
   "metadata": {},
   "source": [
    "## Postlab Questions\n",
    "1. What were your final concentrations with error? How do these errors compare to the standard deviations and 95 % confidencce intervals on the data points themselves? What does this mean about the source(s) of error in this experiment?\n",
    "2. You estimated limits of detection and quantitation for these techniques. Explain what those values mean. How do these values compare to your calculated unknowns?\n",
    "3. Write a brief conclusion - what were our goals and did we meet them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95e420-e1be-4bf0-a0db-d9cd908566b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
